{"cells":[{"cell_type":"markdown","source":["Notebook permettant de charger un fichier parquet dans une table delta.\n\nInitialization: true / false, permet de faire un init / run du chargement. Un init efface le paramètre qui stocke le timestamp du dernier chargement réussi et la table destination dans silver, avant de faire le chargement.\n\nInput Path: chemin abfss du fichier source.\n\nOutput Path: chemin abfss de la table delta destination.\n\nPool: nom du pool utilisé par spark. Les pools sont définis au niveau du cluster dans Advanced Options -> Spark Config (= spark.scheduler.allocation.file /dbfs/FileStore/fairscheduler.xml).\n\nPipeline Name: Nom du pipeline exécutant le notebook.\n\nPipeline Run ID: ID du run du pipeline.\n\nData Factory Name: Nom de la ressource Data Factory exécutant le pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07157f17-98bb-405d-a8c2-dc5cc677f1bf"}}},{"cell_type":"code","source":["%run ../Utils/COMMON"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import des fonctions communes","showTitle":true,"inputWidgets":{},"nuid":"dbab2f31-7d4c-4d8d-bb5d-67e98608dc74"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime\n\ndbutils.widgets.text('input_file_path','abfss://bronze@stdatalake01dev.dfs.core.windows.net/QAD/','Input Path')\ndbutils.widgets.text('output_delta_table','abfss://silver@stdatalake01dev.dfs.core.windows.net/QAD/', 'Output Path')\ndbutils.widgets.text('pool_name', 'byod', 'Pool')\ndbutils.widgets.text('init', 'false', 'Initialization')\n# dbutils.widgets.dropdown(\"encoding\", \"UTF-8\", [\"UTF-8\", \"ISO-8859-1\"], \"Input File Encoding\")\ndbutils.widgets.text('pipeline_run_id', '0', 'Pipeline Run ID')\ndbutils.widgets.text('pipeline_name', '', 'Pipeline Name')\ndbutils.widgets.text('data_factory_name', '', 'Data Factory Name')\ndbutils.widgets.text('folder_partition', 'year=*/month=*/day=*/time=*', 'Folder structure')\n#dbutils.widgets.text('folder_partition', 'year=2021/month=09/day=15/time=1400', 'Folder structure')\n\nstart_date = datetime.utcnow()\n\ninit: bool = (dbutils.widgets.get('init') == 'true')\nfolder_partition: str = dbutils.widgets.get('folder_partition')\n\noutput_full_path: str = dbutils.widgets.get('output_delta_table')\noutput_container, output_storage_account_name, output_file_path = LIN_parse_datalake_url(output_full_path)\nLIN_config_adls_gen2_connector(output_storage_account_name, use_service_principal = True)\n\ninput_full_path: str = dbutils.widgets.get('input_file_path')\ninput_container, input_storage_account_name, input_file_path = LIN_parse_datalake_url(input_full_path)\nLIN_config_adls_gen2_connector(input_storage_account_name, use_service_principal = True)\n\npool_name: str = dbutils.widgets.get('pool_name')\n#encoding: str = dbutils.widgets.get('encoding')\npipeline_run_id: str = dbutils.widgets.get('pipeline_run_id')\npipeline_name: str = dbutils.widgets.get('pipeline_name')\ndata_factory_name: str = dbutils.widgets.get('data_factory_name')\n  \n#monitor= LIN_Monitor(f'CSV_TO_DELTA', data_factory_name, pipeline_run_id, pipeline_name, LIN_get_source(input_full_path), LIN_Stage.SILVER)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Déclaration des widgets et initialisation de variables","showTitle":true,"inputWidgets":{},"nuid":"7e9dc371-da52-4b42-a19e-cb8858e40f76"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if init :\n  # Suppression des tables delta dans silver\n  print(output_full_path)\n  try : \n    dbutils.fs.ls(output_full_path)\n    dbutils.fs.rm(output_full_path, True)\n  except : \n    print(f\"folder {output_full_path} doesn't exist\")\n  # Suppression des paramètres de dernier chargement pour tout recharger.\n  LIN_delete_loading_parameter(input_storage_account_name, input_full_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Dans le cas d'un chargement init","showTitle":true,"inputWidgets":{},"nuid":"9d00ffe2-5822-47a5-a84c-fb4ad9fec4cd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["try:\n  print(\"start reading\")\n\n  print(\"parquet path:\" + input_full_path+folder_partition + '/*.parquet')\n\n  # Lecture du fichier et ajout des colonnes filePath, folderDatetime.\n  df = (\n      spark.read.option('basePath', input_full_path)\n        .format(\"parquet\").load(input_full_path+folder_partition+ '/*.parquet') \n        .withColumn('filePath', f.input_file_name())\n        .withColumn('folderDatetime', f.to_timestamp(f.concat((f.col('year') * 10000 + f.col('month') * 100 + f.col('day')).cast('string'), f.lit(' '), f.format_string(\"%06d\", f.col('time'))), 'yyyyMMdd HHmmss'))\n\n  )\n\n  df.printSchema()\n  df.show()\n  print(\"end reading\")\n  \n  print(\"start load loading parameters \")\n  # Récupérer la date du dernier chargement pour filtrer les répertoires à charger.\n  last_load = LIN_get_loading_parameter(input_storage_account_name, input_full_path)\n  if last_load:\n    df = df.filter(f.col('folderDatetime') > last_load.strftime('%Y-%m-%d %H:%M:%S'))\n  print(\"end load loading parameters \")\n\n  # récupération des noms de fichiers source distincts, permettra de calculer la taille totale des fichier traités.\n  spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", pool_name)\n  input_files = [r['filePath'] for r in df.select('filePath').distinct().collect()]\n  input_files_size: int = sum([LIN_get_dbfs_path_size(af) for af in input_files])\n\n  data_to_load = df.drop('filePath', 'year', 'month', 'day', 'time', 'folderDatetime')\n  \n  # Ajouter les colonnes de metadata\n  data_to_load = LIN_add_metadata_cols(data_to_load)\n  data_to_load = data_to_load.select([f.col(col).alias(col.replace(' ', '')) for col in data_to_load.columns])\n\n  # Ajouter un accumulateur pour compter les lignes sans faire un count.\n  accumulator_count = spark.sparkContext.accumulator(0)\n\n  def LIN_add_to_accu(x):\n    accumulator_count.add(1)\n    return x\n\n  rdd = data_to_load.rdd.map(lambda x: LIN_add_to_accu(x))\n  currated_df = sqlContext.createDataFrame(rdd, data_to_load.schema)\n\n  # Ajouter les colonnes de metadata\n  #currated_df = LIN_add_metadata_cols(data_to_load)\n  #currated_df = currated_df.select([f.col(col).alias(col.replace(' ', '')) for col in currated_df.columns])\n\n  before_write_table_size: int = LIN_get_dbfs_path_size(output_full_path)\n  \n  try :\n    # Définir le pool utilisé.\n    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", pool_name)\n    # Ecriture dans la table delta.\n    currated_df.write.format(\"delta\").mode(\"append\").partitionBy(\"IngestionDate\").save(output_full_path)\n  \n  except:\n    # mettre log\n    raise \n  after_write_table_size: int = LIN_get_dbfs_path_size(output_full_path)\n\n  print(\"start set loading parameters \")\n  # Si il n'y a pas d'erreur pour le chargement des repertoires, on met a jour le parametre du fichier.\n  LIN_set_loading_parameter(input_storage_account_name, input_full_path, start_date)\n  print(\"end set loading parameters \")\n\n  #monitor.log_loading_metrics(LIN_Status.SUCCESS, input_full_path, output_full_path, start_date, datetime.utcnow(), '', accumulator_count.value, input_file_size=input_files_size, output_file_size=(after_write_table_size-before_write_table_size))\n\nexcept:\n  #monitor.log_loading_metrics(LIN_Status.FAILURE, input_full_path, output_full_path, start_date, datetime.utcnow(), LIN_get_error_message())\n  raise "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"276a7033-3f1f-4a8e-aa5a-32b5766c4b96"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce5ebb0-bb8f-4ddd-808d-be77bbf0cc9c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PARQUET_TO_DELTA","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3121234888170039}},"nbformat":4,"nbformat_minor":0}
