{"cells":[{"cell_type":"markdown","source":["Notebook de gestion du monitoring de databricks. \n\nDans le cluster, la librairie \"opencensus-ext-azure==1.0.7\" doit être installée, et la variable d'environnement APPINSIGHTS_CONNEXION_STRING doit exister.\n\nLa classe Monitor permet de faire un log classique, un log avec les métriques pour un chargement de données, un log avec les métriques pour la suppression de doublons."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10102c6e-34e1-46a3-8943-564e677ce633"}}},{"cell_type":"code","source":["import os, sys\nimport logging\nimport time\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\nfrom enum import Enum\nfrom datetime import datetime\n\nENVIRONMENT = os.getenv('ENVIRONMENT')\n\n# Type de log\nclass LIN_LogType(Enum):\n  LOG = 1\n  LOADING_METRICS = 2\n  CLEANING_METRICS = 3\n\n# Niveau de log\nclass LIN_Level(Enum):\n  CRITICAL = 1\n  ERROR = 2\n  WARNING = 3\n  INFO = 4\n  DEBUG = 5\n\n# Etape du log LOADING_METRICS\nclass LIN_Stage(Enum):\n  BRONZE = 1\n  SILVER = 2\n  DATAWAREHOUSE = 3\n  DATAMART = 4\n  NONE = 5\n  \n# Source du log LOADING_METRICS\nclass LIN_Source(Enum):\n  GL_LEGACY = 1\n  KHEOPS = 2\n  MANUEL_REFERENTIEL = 3\n  MANUEL_TRANSCO = 4\n  MANUEL_CORRECTION = 5\n  NONE = 6\n  ARTICLE_DB2 = 7\n  CLIENT_DB2 = 8\n  CLIENT_IDIL = 9\n  CLIENT_MANUEL = 10\n  CLIENT_PARAM_EDI = 11\n  \n# Statut du log LOADING_METRICS\nclass LIN_Status(Enum):\n  SUCCESS = 1\n  FAILURE = 2\n\ndef LIN_get_error_message() -> str:\n  # Retourne le type et la valeur de sys.exc_info.\n  return str(sys.exc_info()[0]) + \" \" + str(sys.exc_info()[1])\n\ndef LIN_get_source(path: str) -> HAC_Source:\n  # Retourne la source en se basant sur un chemin.  \n  if '/GLLegacy/' in path:\n    return HAC_Source.GL_LEGACY\n  elif '/Kheops/' in path:\n    return HAC_Source.KHEOPS\n  elif '/Manuel/Referentiel/' in path:\n    return HAC_Source.MANUEL_REFERENTIEL\n  elif '/Manuel/Transco/' in path:\n    return HAC_Source.MANUEL_TRANSCO\n  elif '/Manuel/Correction/' in path:\n    return HAC_Source.MANUEL_CORRECTION\n  elif '/Article/db2/' in path:\n    return HAC_Source.ARTICLE_DB2\n  elif '/Client/db2/' in path:\n    return HAC_Source.CLIENT_DB2\n  elif '/Client/idil/' in path:\n    return HAC_Source.CLIENT_IDIL\n  elif '/Client/manuel/' in path:\n    return HAC_Source.CLIENT_MANUEL\n  elif '/Client/param_edi/' in path:\n    return HAC_Source.CLIENT_PARAM_EDI\n\n  return HAC_Source.NONE\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baf4647c-1295-4fb7-b1e4-574abf8476f0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class LIN_Monitor():\n  \n  __logger__: logging.Logger\n  __notebook__: str\n  __data_factory_name__: str\n  __pipeline_run_id__: str\n  __pipeline_name__: str\n  __source__: str\n  __stage__: str\n  \n  def __init__(self, notebook: str, data_factory_name:str, pipeline_run_id: str, pipeline_name: str, source: HAC_Source, stage: HAC_Stage):\n    # Constructeur de la classe.\n    # Creation du logger dont le nom est le nom du notebook.\n    # notebook (str), nom du notebook.\n    # data_factory_name (str), nom de la ressource ADF.\n    # pipeline_run_id (str), id de l'exécution du pipeline ADF.\n    # pipeline_name (str), nom du pipeline ADF.\n    # source (HAC_Source), source de données.\n    # stage (HAC_Stage), etape du chargement.\n    \n    if not isinstance(stage, HAC_Stage):\n      raise TypeError('stage parameter must be an instance of HAC_Stage')\n      \n    if not isinstance(source, HAC_Source):\n      raise TypeError('source parameter must be an instance of HAC_Source')\n    \n    self.__notebook__ = notebook\n    self.__source__ = source\n    self.__stage__ = stage\n    self.__data_factory_name__ = data_factory_name\n    self.__pipeline_run_id__ = pipeline_run_id\n    self.__pipeline_name__ = pipeline_name\n    logger_name = \"Monitor_\" + notebook + str(datetime.now())\n\n    if not(logging.root.manager.loggerDict.get(logger_name) is None):\n         print(\"Monitor class has been instantiated twice.\")\n        \n    self.__logger__ = logging.getLogger(logger_name)\n    azure_log_handler = AzureLogHandler(connection_string=os.getenv('APPINSIGHTS_CONNEXION_STRING'), export_interval=0.0)\n    self.__logger__.addHandler(azure_log_handler)\n    \n  def __del__(self): \n    # Destructeur de la classe.\n    # Pour éviter le problème en recette en executant le notebook avec ADF, ou aucun log n'est produit, même avec export_interval=0.0.\n    time.sleep(5)\n    \n  def log(self, level: HAC_Level, message: str):\n    # Creation d'un log basique avec le choix du niveau de log.\n    # level (HAC_Level), niveau de log.\n    # message (str), message du log.\n      \n    if not isinstance(level, HAC_Level):\n      raise TypeError('level parameter must be an instance of HAC_Level')\n    \n    properties = {'custom_dimensions': {\n      'dataFactoryName': self.__data_factory_name__,\n      'environment': ENVIRONMENT, \n      'logType': HAC_LogType.LOG.name, \n      'notebookName': self.__notebook__,\n      'pipelineName': self.__pipeline_name__,\n      'pipelineRunId': self.__pipeline_run_id__}}\n    \n    cleaned_message = message.replace(\"\\n\", \" \")\n    \n    self.__logger__.setLevel(logging.DEBUG)\n    if level == HAC_Level.CRITICAL:\n      self.__logger__.critical(cleaned_message, extra=properties)\n    elif level == HAC_Level.ERROR:\n      self.__logger__.error(cleaned_message, extra=properties)\n    elif level == HAC_Level.WARNING:\n      self.__logger__.warning(cleaned_message, extra=properties)\n    elif level == HAC_Level.INFO:\n      self.__logger__.info(cleaned_message, extra=properties)\n    elif level == HAC_Level.DEBUG:\n      self.__logger__.debug(cleaned_message, extra=properties)\n    self.__logger__.setLevel(logging.WARNING)\n    \n  def log_loading_metrics(self, status: HAC_Status, input_path: str, output_path: str, start_timestamp: datetime, end_timestamp: datetime, message: str, output_rows_count: int=0, input_file_size: int=0, output_file_size: int=0):\n    # Creation d'un log avec les informations d'un chargement de fichier.\n    # status (str), statut du chargement.\n    # input_path (str), chemin source.\n    # output_path (str), chemin destination.\n    # start_timestamp (datetime), date et heure de debut du chargement.\n    # end_timestamp (datetime), date et heure de fin du chargement.\n    # message (int), message d'erreur. \n    # output_rows_count (int), nombre de lignes chargees.\n    # input_file_size (int), taille du fichier source.\n    # output_file_size (int), taille du fichier destination.\n      \n    if not isinstance(status, HAC_Status):\n      raise TypeError('status parameter must be an instance of HAC_Status')\n    \n    cleaned_message = message.replace(\"\\n\", \" \")\n    \n    properties = {'custom_dimensions': {\n      'dataFactoryName': self.__data_factory_name__,\n      'duration': (end_timestamp - start_timestamp).total_seconds(), \n      'endLoading': end_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \n      'environment': ENVIRONMENT, \n      'inputFileSize': input_file_size,\n      'inputPath': input_path, \n      'logType': HAC_LogType.LOADING_METRICS.name, \n      'notebookName': self.__notebook__,\n      'message': cleaned_message,\n      'outputFileSize': output_file_size,\n      'outputPath': output_path,\n      'pipelineName': self.__pipeline_name__,\n      'pipelineRunId': self.__pipeline_run_id__,\n      'rowCount': output_rows_count,\n      'source': self.__source__.name,\n      'stage': self.__stage__.name,\n      'startLoading': start_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n      'status': status.name}}\n    \n    self.__logger__.setLevel(logging.DEBUG)\n    if status == HAC_Status.SUCCESS:\n      self.__logger__.info(HAC_LogType.LOADING_METRICS.name, extra=properties)\n    else:\n      self.__logger__.error(HAC_LogType.LOADING_METRICS.name, extra=properties)\n    self.__logger__.setLevel(logging.WARNING)\n    \n  def log_cleaning_metrics(self, status: HAC_Status, table_path: str, start_timestamp: datetime, end_timestamp: datetime, message: str, deleted_rows_count: int=0):\n    # Creation d'un log avec les informations de suppression d'une table delta.\n    # table_path (str), table nettoyee.\n    # start_timestamp (datetime), date et heure de debut du chargement.\n    # end_timestamp (datetime), date et heure de fin du chargement.\n    # deleted_rows_count (int), nombre de lignes supprimees \n    \n    cleaned_message = message.replace(\"\\n\", \" \")\n    \n    properties = {'custom_dimensions': {\n      'dataFactoryName': self.__data_factory_name__,\n      'duration': (end_timestamp - start_timestamp).total_seconds(), \n      'endLoading': end_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \n      'environment': ENVIRONMENT, \n      'logType': HAC_LogType.CLEANING_METRICS.name, \n      'message': cleaned_message,\n      'notebookName': self.__notebook__,\n      'pipelineName': self.__pipeline_name__,\n      'pipelineRunId': self.__pipeline_run_id__,\n      'rowCount': deleted_rows_count,\n      'startLoading': start_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n      'tablePath': table_path,\n      'status': status.name}}\n    \n    self.__logger__.setLevel(logging.DEBUG)\n    self.__logger__.info(HAC_LogType.CLEANING_METRICS.name, extra=properties)\n    self.__logger__.setLevel(logging.WARNING)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccfa1151-ccfa-4d74-8197-348dc786afe1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MONITOR","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3121234888170071}},"nbformat":4,"nbformat_minor":0}
