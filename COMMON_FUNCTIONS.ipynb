{"cells":[{"cell_type":"markdown","source":["Notebook qui centralise diverses fonctions communes du projet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15536f25-69fb-41f1-9c66-a811de7006f5"}}},{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql import DataFrame\n\ndef LIN_add_metadata_cols(df: DataFrame) -> DataFrame:\n  # Ajouter les colonnes de metadata au dataframe. Les colonnes ajoutées sont: BronzeFileNameSource (le nom du fichier traité), \n  # BronzeFolderSource (chemin du répertoire contenant le fichier traité, démarre et finit avec '/'), IngestionDate (la date UTC de traitement)\n  # df (DataFrame), le dataframe auquel il faut ajouter les colonnes.\n  # retourne un nouveau dataframe avec les colonnes de metadata. \n  \n  currated_df = df.withColumn('absoluteFilePath', f.regexp_replace(f.input_file_name(), r'^abfss://([\\w-]+)@([\\w-]+).dfs.core.windows.net', ''))\n  currated_df = currated_df.withColumn('splittedAbsoluteFilePath', f.split(f.col('absoluteFilePath'), '/'))\n  currated_df = currated_df.withColumn('BronzeFileNameSource', f.col('splittedAbsoluteFilePath')[f.size(f.col('splittedAbsoluteFilePath')) - 1])\n  currated_df = currated_df.withColumn('BronzeFolderSource', f.expr(\"concat('/', concat_ws('/', slice(splittedAbsoluteFilePath, 2, size(splittedAbsoluteFilePath)-2)), '/')\"))\n  currated_df = currated_df.withColumn('IngestionDate', f.current_timestamp())\n  return currated_df.drop('absoluteFilePath', 'splittedAbsoluteFilePath')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"732ab7d5-87a7-4e4f-991b-cb41d994b9aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def LIN_config_adls_gen2_connector(storage_account_name: str, use_service_principal: bool = True ) -> None:\n  # Définir la configuration spark pour lire et écrire sur le adls gen2 storage account avec un service principal.\n  # storage_account_name (str), storage account auquel s'authentifier.  \n  \n  if use_service_principal:\n    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", dbutils.secrets.get(scope=\"scope-datalake\",key=\"sp-adbw-st-datalake-clientid\"))\n    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", dbutils.secrets.get(scope=\"scope-datalake\", key=\"sp-adbw-st-datalake-secret\"))\n    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='scope-datalake',key='sp-adbw-st-datalake-tenantid')}/oauth2/token\")\n  else: \n    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", \"6NNp+F1YV315/lg6oo4si3P83zB149aihPuZha609f6vDRrnHCGgelWflQQT4SUqJlM2kkY0A8O4qVgRlwwzFg==\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbff0fcc-f921-49e1-91bf-04e463c9cb11"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from typing import List\n\ndef LIN_mount_adls_gen2(containers: List[str], storage_account_name: str) -> None:\n  # Monter des containers dans dbfs.\n  # containers (List[str]), liste de containers à monter.\n  # storage_account_name (str), storage account des containers.\n  \n  # Configuration de l'authentification.\n  configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n           \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(scope=\"scope-datalake\",key=\"sp-adbw-st-datalake-clientid\"),\n           \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"scope-datalake\",key=\"sp-adbw-st-datalake-secret\"),\n           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='scope-datalake',key='sp-adbw-st-datalake-tenantid')}/oauth2/token\"}\n  \n  # Lister les containers déja montés.\n  try:\n    mounted_containers = [m.name[:-1] for m in dbutils.fs.ls(\"/mnt/dlk/\")]\n  except: \n    mounted_containers = []\n    \n  # Pour chaque container à monter, vérifier qu'il ne le soit pas déja, sinon le monter.\n  for container in containers:\n    if container not in mounted_containers:\n      dbutils.fs.mount(\n        source = f\"abfss://{container}@{storage_account_name}.dfs.core.windows.net/\",\n        mount_point = f\"/mnt/dlk/{container}\",\n        extra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b7e2ea6-7ba0-4b50-b57d-7d20f03f2f33"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from typing import List\nfrom pyspark.dbutils import FileInfo\n\ndef LIN_get_dbfs_path_size(path: str) -> int:\n  # Calculer récursivement la taille totale d'un répertoire du dbfs. Il peut s'agir d'un emplacement ADLS Gen2 si celui-ci est configuré dans les spark.conf.\n  # path (str) chemin d'un répertoire dont on veut connaitre la taille.\n  # retourne la taille du répertoire.\n\n  def LIN_get_size_recusive(files: List[FileInfo]) -> int:\n    # Calculer la taille totale d'une liste de FileInfo.\n    # files (List[FileInfo]), liste de FileInfo dont on veut connaitre la taille.\n    # retourne la taille de la liste de FileInfo.\n    \n    total_size = 0\n    for file in files:\n      if file.isFile():\n        total_size += file.size\n      else:\n        total_size += LIN_get_size_recusive(dbutils.fs.ls(file.path))\n    return total_size\n\n  try:\n    return LIN_get_size_recusive(dbutils.fs.ls(path))\n  except:\n    return 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e1c9fae-dc1d-4ac6-8eb6-f838489d813d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import re\n\ndef LIN_clean_table_name(input_table: str) -> str:\n  # Nettoyer le nom d'une table en utilisant une regex.\n  # input_table (str), nom de la table à nettoyer.\n  # retourne le nom de la table nettoyé.\n  \n  matches = re.search(r'^(?:\\[?(?P<schema>\\w+)\\]?\\.)?\\[?(?P<tableName>\\w+)\\]?$', input_table)\n  return matches.group('tableName').upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4ca7bc4-f3a5-49ab-b987-bafc2bb9c409"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import re\n\ndef LIN_parse_datalake_url(url: str) -> (str, str, str):\n  # Découper un chemin abfss pour avoir le container, le storage account, le chemin du fichier.\n  # url (str), chemin abfss.\n  # retourne le container, le storage account, le chemin du fichier.\n  \n  search_url = re.search(r\"^abfss://([\\w-]+)@([\\w-]+).dfs.core.windows.net(.*)$\", url)\n  container: str = search_url.group(1)\n  storage_account_name: str = search_url.group(2)\n  file_path: str = search_url.group(3)\n  return container, storage_account_name, file_path"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47c7473f-8b52-4882-8d10-9ce7e7948fe0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def LIN_run_notebook(name: str, params, context=None) -> int:\n  # Démarrer un notebook.\n  # name (str), nom du notebook à démarrer.\n  # params, paramètres du notebook.\n  # context, contexte databricks.\n  # retourne 0 si l'exécution n'a pas eu de problème sinon -1.\n  \n  try:\n    if context:\n      dbutils.notebook.setContext(context)\n    dbutils.notebook.run(name, 0, params)\n    return 0\n  except:\n    return -1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b189840-e088-4f24-a925-405f4a7b8509"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"COMMON_FUNCTIONS","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3121234888170031}},"nbformat":4,"nbformat_minor":0}
